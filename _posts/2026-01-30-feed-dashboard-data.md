---
title: 2026-01-30-feed-dashboard-data
description: 
date: 2026-01-30 05:00:00 -0500
categories: [feed]
tags: [hpc, dataviz, ai, llm]
pin: false
math: true
mermaid: true
pin: false
---

> Using dashboards comes with risks: they leave out critical context by over-simplifying and hence give false certainty. A more nuanced approach including interpreation by experts, and showing multiple perspectives is needed when visualizing data

[The Case Against Dashboards (when Visualizing a Pandemic), Alexander Lex](https://vdl.sci.utah.edu/blog/2020/07/06/dashboards/)

> However, the free-market model did not achieve these
goals. In spite of the prospect of cheap computing
power, relatively few scientists created BOINC
projects. Some of the reasons for this are inherent in
the model. In the free-market model, creating a project
is risky: there's a substantial investment [7], with no
guarantee of any return, since no one may volunteer. 

[Coordinating Volunteer Computing, David P. Anderson](https://scienceunited.org/doc/pearc_20.pdf) 
<https://scienceunited.org>

> Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei](https://arxiv.org/abs/2402.17764)

> The big advances in LLMs in 2025 came from "Reinforcement Learning from Verifiable Rewards" - also known as "reasoning". Andrej Karpathy has a good explanation of that here > <https://karpathy.bearblog.dev/year-in-review-2025/>
> The reason they're so much better at code today than they were ~12 months ago is that the labs found ways to run Reinforcement Learning loops where they generate vast amounts of code, that code is then verified (compiled, tests run etc) and the agents get rewarded for good code.
> And on the individual machine level, I've been finding TDD loops are astonishingly effective with the latest generation of models. You can consider that a learning loop - I even sometimes have them write notes about what they learned at the end - although aside from those notes the slate gets cleared every time you start a new session.

[Simon Willison](https://lobste.rs/s/cmsfbu/don_t_fall_into_anti_ai_hype)

> Tokens all the way down. People used to compare humans to computers and before that to machines. Those analogies fell short and this one will too
[Moltbook is the most interesting place on the internet right now, HackerNews](https://news.ycombinator.com/item?id=46826963)

>As a developer with a bottomless wishlist of things I wish I could have done or tried out, I have been able to use LLM tools to not just rapidly prototype and validate complex ideas, but actually write good quality production-grade software (my own subjective metric, of course) with better code than I could have written manually—things where I knew exactly what I had to do, but was constrained by physical limits, and also things that were unclear to me and needed novel ideas, approaches, and leaps. All the while, learning and bettering my own understanding of things.
> ...
>Remember the old adage, “programming is 90% thinking and 10% typing”? It is now, for real.
> ...
> The reality is that the significant majority of the code written by humans globally on a daily basis, is likely borderline junk.[12] Software development is not even a discipline that has reached any objective level of maturity. Medical doctors and civil engineers go through rigorous training to be issued licenses that are contingent on real world ramifications of their work. How about software developers and engineers?
> ...
> Code was always a means to an end. Unlike poetry or prose, end users don’t read or care about code. They don’t care what language or framework or the architecture the hundred systems running behind a portal are made of. Code is hidden. They interact with the effect and outcomes of code through various forms of UX.
> ...
> FOSS is perhaps the greatest public commons that humanity has created. The genesis of FOSS and its predecessors, various schemes for sharing code, can be traced to the fundamental premise that software was prohibitively expensive and required immense specialist skills to create.
>...
>Soon, one is stuck with a codebase whose workings one doesn’t understand, and one is forced to go back to the genie and depend on it helplessly. And because one is hooked on and dependent on the genie, the natural circumstances that otherwise would allow for foundational and fundamental skills and understanding to develop, never arise, to the point of cognitive decline
[Code is cheap. Show me the talk. Kailash Nadh's](https://nadh.in/blog/code-is-cheap/)

<https://jslegenddev.substack.com/p/you-should-make-web-games>

> Moral of the story: even insane-looking problems are sometimes real. 
[Ice Cream Cone Car](https://www.snopes.com/fact-check/cone-of-silence/)

> We’ve already seen entire categories begin to be eaten up by AI: Stack Overflow and Chegg were early victims, but now we’re seeing pressure on new sectors.
[Software Survival 3.0, Steve Yegge](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)