---
title: 2026-01-30-feed-dashboard-data
description: 
date: 2026- 05:00:00 -0500
categories: [feed]
tags: [hpc, dataviz, ai, llm]
pin: false
math: true
mermaid: true
pin: false
---

> Using dashboards comes with risks: they leave out critical context by over-simplifying and hence give false certainty. A more nuanced approach including interpreation by experts, and showing multiple perspectives is needed when visualizing data

[The Case Against Dashboards (when Visualizing a Pandemic), Alexander Lex](https://vdl.sci.utah.edu/blog/2020/07/06/dashboards/)

> However, the free-market model did not achieve these
goals. In spite of the prospect of cheap computing
power, relatively few scientists created BOINC
projects. Some of the reasons for this are inherent in
the model. In the free-market model, creating a project
is risky: there's a substantial investment [7], with no
guarantee of any return, since no one may volunteer. 

[Coordinating Volunteer Computing, David P. Anderson](https://scienceunited.org/doc/pearc_20.pdf) 
<https://scienceunited.org>

> Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei](https://arxiv.org/abs/2402.17764)