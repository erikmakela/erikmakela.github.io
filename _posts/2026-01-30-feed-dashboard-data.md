---
title: 2026-01-30-feed-dashboard-data
description: 
date: 2026- 05:00:00 -0500
categories: [feed]
tags: [hpc, dataviz, ai, llm]
pin: false
math: true
mermaid: true
pin: false
---

> Using dashboards comes with risks: they leave out critical context by over-simplifying and hence give false certainty. A more nuanced approach including interpreation by experts, and showing multiple perspectives is needed when visualizing data

[The Case Against Dashboards (when Visualizing a Pandemic), Alexander Lex](https://vdl.sci.utah.edu/blog/2020/07/06/dashboards/)

> However, the free-market model did not achieve these
goals. In spite of the prospect of cheap computing
power, relatively few scientists created BOINC
projects. Some of the reasons for this are inherent in
the model. In the free-market model, creating a project
is risky: there's a substantial investment [7], with no
guarantee of any return, since no one may volunteer. 

[Coordinating Volunteer Computing, David P. Anderson](https://scienceunited.org/doc/pearc_20.pdf) 
<https://scienceunited.org>

> Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective.

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei](https://arxiv.org/abs/2402.17764)

> The big advances in LLMs in 2025 came from "Reinforcement Learning from Verifiable Rewards" - also known as "reasoning". Andrej Karpathy has a good explanation of that here > <https://karpathy.bearblog.dev/year-in-review-2025/>
> The reason they're so much better at code today than they were ~12 months ago is that the labs found ways to run Reinforcement Learning loops where they generate vast amounts of code, that code is then verified (compiled, tests run etc) and the agents get rewarded for good code.
> And on the individual machine level, I've been finding TDD loops are astonishingly effective with the latest generation of models. You can consider that a learning loop - I even sometimes have them write notes about what they learned at the end - although aside from those notes the slate gets cleared every time you start a new session.

[Simon Willison](https://lobste.rs/s/cmsfbu/don_t_fall_into_anti_ai_hype)